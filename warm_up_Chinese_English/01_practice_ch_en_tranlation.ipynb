{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation\n",
    "\n",
    "Huggingface transformer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Huggingface tokenizer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Useful resources from huggingface -- fine-tuning a model from scratch: https://huggingface.co/blog/how-to-train\n",
    "\n",
    "The code I wrote before might be helpful: https://github.com/submal/ctec-lambus/blob/master/xprmt/xprmt_06.ipynb\n",
    "\n",
    "A code example of fine-tuning T5 for text summarization: https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81 \n",
    "\n",
    "LighningModule API\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-apihttps://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import nlp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3456748923243064</td>\n",
       "      <td>如果你真要梦想成真，就先从梦中醒来。</td>\n",
       "      <td>If you really want to dream come true, first w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3434266673334383</td>\n",
       "      <td>如果别人朝你扔石头，就不要扔回去了，留着作你建高楼的基石。大家早晨好</td>\n",
       "      <td>If they throw stones at you, don't throw back,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3507444192051677</td>\n",
       "      <td>这么早起，我都不是我了</td>\n",
       "      <td>good morning everybody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>10783626359</td>\n",
       "      <td>外国人眼中的中国禁烟</td>\n",
       "      <td>'This is China' no excuse for defying smoking ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>15861401364</td>\n",
       "      <td>舞蹈是隐藏在灵魂中的一种语言。</td>\n",
       "      <td>is the hidden language of the soul.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                              source  \\\n",
       "1998  3456748923243064                  如果你真要梦想成真，就先从梦中醒来。   \n",
       "1999  3434266673334383  如果别人朝你扔石头，就不要扔回去了，留着作你建高楼的基石。大家早晨好   \n",
       "2000  3507444192051677                         这么早起，我都不是我了   \n",
       "2001       10783626359                          外国人眼中的中国禁烟   \n",
       "2002       15861401364                     舞蹈是隐藏在灵魂中的一种语言。   \n",
       "\n",
       "                                                 target  \n",
       "1998  If you really want to dream come true, first w...  \n",
       "1999  If they throw stones at you, don't throw back,...  \n",
       "2000                             good morning everybody  \n",
       "2001  'This is China' no excuse for defying smoking ban  \n",
       "2002                is the hidden language of the soul.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "# Load and shuffle data\n",
    "weiboDf= pd.DataFrame(weiboDict).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.544 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['哪怕', '是', '世界末日', ',', '我', '都', '会', '爱', '你', '。', '[', '心', ']', ' ', '喜欢', '就', '关注']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in chTexts:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "# Store all English text in a single file \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTexts: \n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece\n",
    "\n",
    "https://github.com/huggingface/tokenizers\n",
    "\n",
    "In the following cell, we train a `SentencePiece` tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4520, 474, 560, 6049, 6014, 1094, 1582, 1311, 406, 1035] ['▁哪', '怕', '是', '世界末日', ',我都会', '爱你', '。[心]', '▁喜欢', '就', '关注'] [(0, 1), (1, 2), (2, 3), (3, 7), (7, 11), (11, 13), (13, 17), (17, 20), (20, 21), (21, 23)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\myTokenizer-vocab.json', '.\\\\myTokenizer-merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "chTokenizer.train([pathAllCh], \n",
    "                vocab_size = 20000, \n",
    "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = chTokenizer.encode(chTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "chTokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to encode a list of texts as a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁哪', '怕', '是', '世界末日', ',我都会', '爱你', '。[心]', '▁喜欢', '就', '关注']\n",
      "['▁距离', '并不', '可怕', ',可', '怕', '的是', '心', '越来越', '远']\n",
      "['▁不', '敢', '相信', ',我', '直到', '现在', '才', '翻唱', '了一', '首', 'J', 'B', '的歌', '。我', '非常', '开心', '你们', '喜欢']\n"
     ]
    }
   ],
   "source": [
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: padding and truncation\n",
    "\n",
    "Huggingface tokenizer allows us to pad or truncate according to a length. The following are common utilities for padding and truncation: \n",
    "\n",
    "`Tokenizer.enable_padding(**args)` -- Enable padding\n",
    "\n",
    "`Tokenizer.padding` -- Info about padding\n",
    "\n",
    "`Tokenizer.no_padding()` -- Disable padding\n",
    "\n",
    "`Tokenizer.enable_truncation(**args)` -- Enable truncation \n",
    "\n",
    "`Tokenizer.truncation` -- Info about truncation \n",
    "\n",
    "`Tokenizer.no_truncation()` -- Disable truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁哪', '怕', '是', '世界末日', ',我都会', '爱你', '。[心]', '▁喜欢', '就', '关注', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['▁距离', '并不', '可怕', ',可', '怕', '的是', '心', '越来越', '远', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['▁不', '敢', '相信', ',我', '直到', '现在', '才', '翻唱', '了一', '首', 'J', 'B', '的歌', '。我', '非常', '开心', '你们', '喜欢']\n",
      "{'length': 15, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    }
   ],
   "source": [
    "# With Padding\n",
    "chTokenizer.enable_padding(length = 15)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁哪', '怕', '是', '世界末日', ',我都会', '爱你', '。[心]', '▁喜欢', '就', '关注']\n",
      "['▁距离', '并不', '可怕', ',可', '怕', '的是', '心', '越来越', '远']\n",
      "['▁不', '敢', '相信', ',我', '直到', '现在', '才', '翻唱', '了一', '首', 'J', 'B', '的歌', '。我', '非常', '开心', '你们', '喜欢']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No padding\n",
    "chTokenizer.no_padding()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁哪', '怕', '是']\n",
      "['▁距离', '并不', '可怕']\n",
      "['▁不', '敢', '相信']\n",
      "{'max_length': 3, 'stride': 0, 'strategy': 'longest_first'}\n"
     ]
    }
   ],
   "source": [
    "# With truncation\n",
    "chTokenizer.enable_truncation(max_length = 3)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁哪', '怕', '是', '世界末日', ',我都会', '爱你', '。[心]', '▁喜欢', '就', '关注']\n",
      "['▁距离', '并不', '可怕', ',可', '怕', '的是', '心', '越来越', '远']\n",
      "['▁不', '敢', '相信', ',我', '直到', '现在', '才', '翻唱', '了一', '首', 'J', 'B', '的歌', '。我', '非常', '开心', '你们', '喜欢']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No truncation\n",
    "chTokenizer.no_truncation()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Pending problem.</span> As I tried to follow the tutorial and load the tokenizer saved on disk, unexpected error was reported. For now, skip loading saved tokenizer and proceed with other important steps.  \n",
    "\n",
    "<span style=\"color:red;\">Bottleneck for now.</span> Do we need special token for T5? If yes, how to insert special T5 tokens into our tokenization? Similar to `tokenizers.processors.BertProcessing`, do we have `tokenizers.processors.T5Processing`? \n",
    "\n",
    "<span style=\"color:red;\">Solution.</span> 1. Thoroughtly read documentation for T5 model in huggingface doc; 2. Explore `huggingface/tokenizers` library on github. \n",
    "\n",
    "For now, halt with tokenizer and proceed with language model until bumping into problems. Keep in mind the confusion about special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1338, 4066, 910, 874, 918, 959, 918, 875, 872, 3267, 1080, 1422, 884, 6148, 1869, 2896, 47, 961, 1319] ['▁In', '▁spite', '▁of', '▁you', '▁and', '▁me', '▁and', '▁the', '▁s', 'illy', '▁world', '▁going', '▁to', '▁pieces', '▁around', '▁us,', 'I', '▁love', '▁you.'] [(0, 2), (2, 8), (8, 11), (11, 15), (15, 19), (19, 22), (22, 26), (26, 30), (30, 32), (32, 36), (36, 42), (42, 48), (48, 51), (51, 58), (58, 65), (65, 69), (69, 70), (70, 75), (75, 80)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "enTokenizer.train([pathAllEn], \n",
    "                vocab_size = 20000, \n",
    "               special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = enTokenizer.encode(enTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "# tokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before training\n",
    "\n",
    "To utilize PyTorch and GPU computation, we need to create instances of `Dataset` object. \n",
    "\n",
    "`DataLoader` class allows us to iterate a dataset with given batch size. We will define dataloaders in `T5FineTuner` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell overwrites `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Load original data and util from memory or file\n",
    "    Texts must be passed as lists \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 chTexts, enTexts, # Suppose the two colums are of the same length\n",
    "                 chTokenizer, enTokenizer, \n",
    "                 chMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        chOutputs = chTokenizer.encode(chTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        chEncoding = chOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        chMask = chOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(chEncoding).to(device), \n",
    "            'source_mask': torch.tensor(chMask).to(device), \n",
    "            'target_ids': torch.tensor(enEncoding).to(device), \n",
    "            'target_mask': torch.tensor(enMask).to(device)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "chMaxLen = 100\n",
    "enMaxLen = 100\n",
    "\n",
    "dataset = MyDataset(chTexts[:1500], enTexts[:1500], \n",
    "                    chTokenizer, enTokenizer, \n",
    "                    chMaxLen = chMaxLen, enMaxLen = enMaxLen)\n",
    "\n",
    "print(len(dataset))\n",
    "# print(dataset.__getitem__(0))\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "PyTorch native, despite its great flexibility, may trap you in detailed errors that mess up the entire code. For example, you may forget important details like `optimizer.zero_grad()` or `tensor.to(device)` in PyTorch native. For both learning purpose and clarity in the long run, we use `pytorch_lightning` to define model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams): \n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams.pretrainedModelName, \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.chTokenizer = hparams.chTokenizer\n",
    "        self.enTokenizer = hparams.enTokenizer\n",
    "        # self.rouge_metric = nlp.load_metric('rouge')\n",
    "        \n",
    "        # No idea what the \"freeze\" is doing\n",
    "        if self.hparams.freeze_embed:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams.freeze_encoder:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "            \n",
    "            \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask = None, \n",
    "                decoder_input_ids = None, \n",
    "                decoder_attention_mask = None, \n",
    "                lm_labels = None\n",
    "               ): \n",
    "        # Type `Seq2SeqLMOutput`\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            lm_labels = lm_labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Prepare optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        model = self.model \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # model.named_parameters() can't find doc?\n",
    "                'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay': self.hparams.weight_decay\n",
    "            }, \n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr = self.hparams.learning_rate\n",
    "        )\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    \n",
    "    # Override this method to adjust how Trainer calls each optimizer \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure = None, using_native_amp = False): \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    # Why do we set zero_grad at this moment? \n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 4: Define training logic\n",
    "    -- In PyTorch native, we have to manually define the epoch loop, define the batch loop, and manually perform model.train(), loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    # subroutine for training_step()\n",
    "    def _step(self, batch): \n",
    "        lm_labels = batch['target_ids']    # !! Does not apply! \n",
    "        lm_labels[lm_lables[:, ] == 0] = -100    # !! Verify that id for pad is 0 \n",
    "         \n",
    "        # !! There is a `__call__` method associated with self ?! \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'],    # !! Does not apply! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            lm_labels = lm_labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss    # !! Or should it be outputs[0] ? \n",
    "    \n",
    "    \n",
    "    # Called at the end of training epoch \n",
    "    # Do something with all the outputs from every training step \n",
    "    def training_epoch_end(self, outputs): \n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'avg_train_loss': avg_train_loss}\n",
    "        return {\n",
    "            'avg_train_loss': avg_train_loss, \n",
    "            'log': tensorboard_logs, \n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 5: Define validation logic\n",
    "    -- In PyTorch native, we have to define the batch loop, and manually perform model.eval(), torch.no_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        print('val', end = ', ')\n",
    "        return self._generative_step(batch)\n",
    "    \n",
    "    # subroutine for validation_step()\n",
    "    def _generative_step(self, batch): \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # !! model.generate() Can't find doc !\n",
    "        generated_ids = self.model.generate(\n",
    "            batch['source_ids'],    # !! Does not apply ! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            use_cache = True, \n",
    "            decoder_attention_mask = batch['target_mask'],     # !! Does not apply! \n",
    "            max_length = self.hparams['max_output_len'],\n",
    "            num_beams = 2,     # ?? What is this?\n",
    "            repetition_penalty = 2.5,     # ?? What is this?\n",
    "            length_penalty = 1.0,     # ?? What is this?\n",
    "            early_stopping = True    # ?? What is this?\n",
    "        )\n",
    "        \n",
    "        preds = self.ids_to_clean_text(generated_ids)    # translation predicted by model \n",
    "        target = self.ids_to_clean_text(batch['target_ids'])    # !! Does not apply! \n",
    "        \n",
    "        gen_time = (time.time() - t0) / batch['source_ids'].shape[0]    # !! Does not apply\n",
    "        \n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        # Compute metrics\n",
    "        # ?? What is the deal with \"rouge\" in the code example? \n",
    "        base_metrics = {'val_loss': loss}\n",
    "        trans_len = np.mean(list(map(len, generated_ids)))\n",
    "        base_metrics.update(\n",
    "            gen_time = gen_time, \n",
    "            gen_len = trans_len, \n",
    "            preds = preds, \n",
    "            target = target\n",
    "        )\n",
    "        # self.rouge_metric.add_batch(preds, target)\n",
    "        \n",
    "        return base_metrics\n",
    "        \n",
    "    \n",
    "    #\n",
    "    def validation_epoch_end(self, outputs): \n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        \n",
    "        # rouge_results = self.rouge_metric.compute()\n",
    "        # rouge_dict = self.parse_score(rouge_results)\n",
    "        \n",
    "        tensorboard_logs.update(rouge1 = rouge_dict['rouge1'], rougeL = rouge_dict['rougeL'])\n",
    "        \n",
    "        # Clear out the lists for next epoch \n",
    "        # !! I don't see those two variables defined anywhere \n",
    "        self.target_gen = []\n",
    "        self.prediction_gen = []\n",
    "        \n",
    "        return {\n",
    "            'avg_val_loss': avg_loss, \n",
    "            # 'rouge1': rouge_results['rouge1'], \n",
    "            # 'rougeL': rouge_results['rougeL'], \n",
    "            'log': tensorboard_logs,\n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''Part 6: Define dataloaders'''\n",
    "    def train_dataloader(self): \n",
    "        train_dataset = get_dataset(\n",
    "            chTexts = chTexts[:1800], \n",
    "            enTexts = enTexts[:1800], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams.max_input_len, \n",
    "            enMaxLen = self.hparams.max_output_len\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = self.hparams.train_batch_size, \n",
    "            drop_last = True, \n",
    "            shuffle = True, \n",
    "            num_workers = 0 \n",
    "        )\n",
    "        \n",
    "        # The code below deals with scheduler. And I have no idea what the code is doing \n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            chTexts = chTexts[1800:1950], \n",
    "            enTexts = enTexts[1800:1950], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams.max_input_len, \n",
    "            enMaxLen = self.hparams.max_output_len\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size = self.hparams.eval_batch_size, \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = get_dataset(\n",
    "            chTexts = chTexts[1950:], \n",
    "            enTexts = enTexts[1950:], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams.max_input_len, \n",
    "            enMaxLen = self.hparams.max_output_len\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size = self.hparams.eval_batch_size, \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' ==================================\n",
    "    # Collection of helper functions \n",
    "    # Not predefined by LightningModule\n",
    "    ===================================='''\n",
    "    \n",
    "    # Decode a batch of ids and return a list of strings \n",
    "    def ids_to_clean_text(self, tokenizer, ids_batch): \n",
    "        ids_batch_tensor = torch.tensor(ids_batch)\n",
    "        # Make sure that the ids come as a batch and that decode_batch() method will work properly \n",
    "        assert (id_batch_tensor.ndim >= 2), 'Ids do not form a batch'\n",
    "        return tokenizer.decode_batch(ids_batch.tolist())\n",
    "            \n",
    "        \n",
    "    # tqdm is a library for showing progress bar \n",
    "    # Retrieve info needed for progresse bar \n",
    "    def get_tqdm_dict(self): \n",
    "        # !! What is self.trainer? I never saw it defined \n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.3f}'.format(self.trainer.avg_loss), \n",
    "            'lr': self.lr_scheduler.get_last_lr()[-1]\n",
    "        }\n",
    "        return tqdm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparamsDict = {\n",
    "    'chTokenizer': chTokenizer, \n",
    "    'enTokenizer': enTokenizer, \n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'weight_decay': 0.0, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'train_batch_size': 8, \n",
    "    'eval_batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'n_gpu': 1\n",
    "    # For now, we do train-test split manually when defining dataloader, instead of loading the following param \n",
    "    # 'n_train': 2000\n",
    "    # 'n_val': 150\n",
    "    # 'n_test': 50\n",
    "}\n",
    "\n",
    "# ?? What are these hyperparameters ? \n",
    "hparamsDictNoUnderstand = {\n",
    "    'freeze_encoder': False, \n",
    "    'freeze_embed': False, \n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_stpes': 0,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'resume_from_checkpoint': None, \n",
    "    'val_check_interval': 0.05,\n",
    "    'early_stop_callback': False, \n",
    "    'fp_16': False, \n",
    "    'opt_level': 'O1', \n",
    "    'max_grad_norm': 1.0, \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "hparamsDict.update(hparamsDictNoUnderstand)\n",
    "\n",
    "hparams = argparse.Namespace(**hparamsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea what the following code cells are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "            # Log and save results to file\n",
    "            output_test_results_file = \"./test_results.txt\"\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Checkpoint directory ./ exists and is not empty. With save_top_k=3, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Set up wandb \n",
    "os.environ[\"WANDB_API_KEY\"] = '4a50a6213c69c6a669deb96f81ced074ecac908a'\n",
    "wandb_logger = WandbLogger(project='ch-en-try')\n",
    "\n",
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath='./', prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
    ")\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=hparams.gradient_accumulation_steps,\n",
    "    gpus=hparams.n_gpu,\n",
    "    max_epochs=hparams.num_train_epochs,\n",
    "    # early_stop_callback=False,\n",
    "    precision= 16 if hparams.fp_16 else 32,\n",
    "    amp_level=hparams.opt_level,\n",
    "    resume_from_checkpoint=hparams.resume_from_checkpoint,\n",
    "    gradient_clip_val=hparams.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    val_check_interval=hparams.val_check_interval,\n",
    "    # logger=wandb_logger,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen):\n",
    "    return MyDataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cad06917cc74d699980aa7497820f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val, "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ids_to_clean_text() missing 1 required positional argument: 'ids_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7e71abd870b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5FineTuner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'on_fit_start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\accelerators\\gpu_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# train or test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[1;34m(self, ref_model)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;31m# run eval step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;31m# allow no returns from eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[1;34m(self, test_mode, max_batches)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m                 \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[1;34m(self, test_mode, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;31m# track batch size for weighted average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\accelerators\\gpu_accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__validation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__validation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\accelerators\\gpu_accelerator.py\u001b[0m in \u001b[0;36m__validation_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-49d82abb924e>\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generative_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m# subroutine for validation_step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-49d82abb924e>\u001b[0m in \u001b[0;36m_generative_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mids_to_clean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# translation predicted by model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mids_to_clean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# !! Does not apply!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ids_to_clean_text() missing 1 required positional argument: 'ids_batch'"
     ]
    }
   ],
   "source": [
    "model = T5FineTuner(hparams)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
