{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation\n",
    "\n",
    "Huggingface transformer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Huggingface tokenizer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Useful resources from huggingface -- fine-tuning a model from scratch: https://huggingface.co/blog/how-to-train\n",
    "\n",
    "The code I wrote before might be helpful: https://github.com/submal/ctec-lambus/blob/master/xprmt/xprmt_06.ipynb\n",
    "\n",
    "A code example of fine-tuning T5 for text summarization: https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Model, AdamW\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3477898759956095</td>\n",
       "      <td>能发现自己的错误是智慧，能改正自己的错误是勇敢。喜欢请关注</td>\n",
       "      <td>Can find their own mistakes is wisdom, to corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3526354320111701</td>\n",
       "      <td>我的死后日願望是，可以有世界末日啦！@browNsugaR 我在:</td>\n",
       "      <td>Very good music in Suns [good] //@browNsugaR:E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3558553027867596</td>\n",
       "      <td>年后，让你觉得更失望的不是你做过的事情，而是你没有做过的事情。所以，一直想做的事情，不要再拖了</td>\n",
       "      <td>online#Twenty years from now you will be more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3482173493644453</td>\n",
       "      <td>说：“在小寨军区侧门，有人维权。过来个军车就给人家看。不过貌似四医大不归省军区管吧</td>\n",
       "      <td>It's hard for her to win,she should explicit m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>3562778084139688</td>\n",
       "      <td>没有受伤，不懂坚强；不犯错误，难以成长；未曾失败，何来成功。</td>\n",
       "      <td>Day78：You'll never be brave if you don't get h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                           source  \\\n",
       "1998  3477898759956095                    能发现自己的错误是智慧，能改正自己的错误是勇敢。喜欢请关注   \n",
       "1999  3526354320111701                我的死后日願望是，可以有世界末日啦！@browNsugaR 我在:   \n",
       "2000  3558553027867596  年后，让你觉得更失望的不是你做过的事情，而是你没有做过的事情。所以，一直想做的事情，不要再拖了   \n",
       "2001  3482173493644453        说：“在小寨军区侧门，有人维权。过来个军车就给人家看。不过貌似四医大不归省军区管吧   \n",
       "2002  3562778084139688                   没有受伤，不懂坚强；不犯错误，难以成长；未曾失败，何来成功。   \n",
       "\n",
       "                                                 target  \n",
       "1998  Can find their own mistakes is wisdom, to corr...  \n",
       "1999  Very good music in Suns [good] //@browNsugaR:E...  \n",
       "2000  online#Twenty years from now you will be more ...  \n",
       "2001  It's hard for her to win,she should explicit m...  \n",
       "2002  Day78：You'll never be brave if you don't get h...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "weiboDf = pd.DataFrame(weiboDict)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.691 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好', '的', '爱情', '使', '你', '通过', '一个', '人', '看到', '整个', '世界', '，', '坏', '的', '爱情', '使', '你', '为了', '一个', '人', '舍弃', '整个', '世界', '。']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in chTexts:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "# Store all English text in a single file \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTexts: \n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece\n",
    "\n",
    "https://github.com/huggingface/tokenizers\n",
    "\n",
    "In the following cell, we train a `SentencePiece` tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6941, 1490, 2253, 1036, 1158, 2195, 3013, 339, 2037, 1490, 1197, 1036, 5754, 4099, 94] ['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。'] [(0, 4), (4, 6), (6, 8), (8, 11), (11, 13), (13, 15), (15, 18), (18, 19), (19, 22), (22, 24), (24, 26), (26, 29), (29, 31), (31, 35), (35, 36)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\myTokenizer-vocab.json', '.\\\\myTokenizer-merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "chTokenizer.train([pathAllCh], \n",
    "                vocab_size = 20000, \n",
    "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = chTokenizer.encode(chTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "chTokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to encode a list of texts as a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: padding and truncation\n",
    "\n",
    "Huggingface tokenizer allows us to pad or truncate according to a length. The following are common utilities for padding and truncation: \n",
    "\n",
    "`Tokenizer.enable_padding(**args)` -- Enable padding\n",
    "\n",
    "`Tokenizer.padding` -- Info about padding\n",
    "\n",
    "`Tokenizer.no_padding()` -- Disable padding\n",
    "\n",
    "`Tokenizer.enable_truncation(**args)` -- Enable truncation \n",
    "\n",
    "`Tokenizer.truncation` -- Info about truncation \n",
    "\n",
    "`Tokenizer.no_truncation()` -- Disable truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "{'length': 15, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    }
   ],
   "source": [
    "# With Padding\n",
    "chTokenizer.enable_padding(length = 15)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No padding\n",
    "chTokenizer.no_padding()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过']\n",
      "['▁如果你', '真', '要梦想']\n",
      "['▁有时候,', '最好的', '方法就是']\n",
      "{'max_length': 3, 'stride': 0, 'strategy': 'longest_first'}\n"
     ]
    }
   ],
   "source": [
    "# With truncation\n",
    "chTokenizer.enable_truncation(max_length = 3)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No truncation\n",
    "chTokenizer.no_truncation()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Pending problem.</span> As I tried to follow the tutorial and load the tokenizer saved on disk, unexpected error was reported. For now, skip loading saved tokenizer and proceed with other important steps.  \n",
    "\n",
    "<span style=\"color:red;\">Bottleneck for now.</span> Do we need special token for T5? If yes, how to insert special T5 tokens into our tokenization? Similar to `tokenizers.processors.BertProcessing`, do we have `tokenizers.processors.T5Processing`? \n",
    "\n",
    "<span style=\"color:red;\">Solution.</span> 1. Thoroughtly read documentation for T5 model in huggingface doc; 2. Explore `huggingface/tokenizers` library on github. \n",
    "\n",
    "For now, halt with tokenizer and proceed with language model until bumping into problems. Keep in mind the confusion about special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2101, 961, 1407, 874, 1145, 875, 1771, 1080, 1103, 1016, 1140, 1581, 1459, 961, 1407, 874, 3558, 875, 1771, 1080, 925, 1016, 1140, 20] ['▁Good', '▁love', '▁makes', '▁you', '▁see', '▁the', '▁whole', '▁world', '▁from', '▁one', '▁person', '▁while', '▁bad', '▁love', '▁makes', '▁you', '▁abandon', '▁the', '▁whole', '▁world', '▁for', '▁one', '▁person', '.'] [(0, 4), (4, 9), (9, 15), (15, 19), (19, 23), (23, 27), (27, 33), (33, 39), (39, 44), (44, 48), (48, 55), (55, 61), (61, 65), (65, 70), (70, 76), (76, 80), (80, 88), (88, 92), (92, 98), (98, 104), (104, 108), (108, 112), (112, 119), (119, 120)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "enTokenizer.train([pathAllEn], \n",
    "                vocab_size = 20000, \n",
    "               special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = enTokenizer.encode(enTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "# tokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before training\n",
    "\n",
    "To utilize PyTorch and GPU computation, we need to create instances of `Dataset` object. \n",
    "\n",
    "`DataLoader` class allows us to iterate a dataset with given batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell overwrites `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Load original data and util from memory or file\n",
    "    Texts must be passed as lists \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 chTexts, enTexts, # Suppose the two colums are of the same length\n",
    "                 chTokenizer, enTokenizer, \n",
    "                 chMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        chOutputs = chTokenizer.encode(chTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        chEncoding = chOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        chAttention = chOutputs.attention_mask\n",
    "        enAttention = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'ch': {\n",
    "                'ids': torch.tensor(chEncoding).to(device), \n",
    "                'attention_mask': torch.tensor(chAttention).to(device)\n",
    "            }, \n",
    "            'en': {\n",
    "                'ids': torch.tensor(enEncoding).to(device), \n",
    "                'attention_mask': torch.tensor(enAttention).to(device)\n",
    "            } \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `Dataset` and `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 70\n",
    "\n",
    "dataset = MyDataset(chTexts, enTexts, \n",
    "                    chTokenizer, enTokenizer, \n",
    "                    chMaxLen = MAX_LEN, enMaxLen = MAX_LEN)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2260, 915, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ['▁我在', '这', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Verify the tokenizer is working\n",
    "\n",
    "output = dataset.chTokenizer.encode('我在这')\n",
    "\n",
    "print(output.ids, output.tokens, output.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load T5 model\n",
    "model = T5Model.from_pretrained(\n",
    "    't5-small', return_dict = True\n",
    ").to(device)\n",
    "\n",
    "# Set to training mode\n",
    "model.train(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to either use PyTorch native or `Trainer` class in huggingface to fine-tune the model. For now we use PyTorch native. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
