{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation\n",
    "\n",
    "Huggingface transformer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Huggingface tokenizer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Useful resources from huggingface -- fine-tuning a model from scratch: https://huggingface.co/blog/how-to-train\n",
    "\n",
    "The code I wrote before might be helpful: https://github.com/submal/ctec-lambus/blob/master/xprmt/xprmt_06.ipynb\n",
    "\n",
    "A code example of fine-tuning T5 for text summarization: https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81 \n",
    "\n",
    "LighningModule API\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-apihttps://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import nlp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>9992913096</td>\n",
       "      <td>如果只是遇见，不能停留，不如不遇见</td>\n",
       "      <td>so the most important thing is to be togetherI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>11062569546</td>\n",
       "      <td>上个周末很特别呢，嘻嘻，没有考试。于是平时一直和烤鸭们并肩作战的同事终于有机会聚到一起，te...</td>\n",
       "      <td>What an amazingly hard working, professional a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3445340974127063</td>\n",
       "      <td>最后和你在一起的人，往往是你想不到的人</td>\n",
       "      <td>Finally,people with you,often you can not thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3466028057423362</td>\n",
       "      <td>年轻的时候会想要谈很多次恋爱，但是随着年龄的增长，终于领悟到爱一个人，就算用一辈子的时间，还...</td>\n",
       "      <td>we are young, we may want several love experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>3489408630304771</td>\n",
       "      <td>每个人都会累，没人能为你承担所有的伤悲，人总有那么一段时间要学会自己长大。更多新东方VIP留...</td>\n",
       "      <td>gets tired.No one can take the pain for you. Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             source  \\\n",
       "1998        9992913096                                  如果只是遇见，不能停留，不如不遇见   \n",
       "1999       11062569546  上个周末很特别呢，嘻嘻，没有考试。于是平时一直和烤鸭们并肩作战的同事终于有机会聚到一起，te...   \n",
       "2000  3445340974127063                                最后和你在一起的人，往往是你想不到的人   \n",
       "2001  3466028057423362  年轻的时候会想要谈很多次恋爱，但是随着年龄的增长，终于领悟到爱一个人，就算用一辈子的时间，还...   \n",
       "2002  3489408630304771  每个人都会累，没人能为你承担所有的伤悲，人总有那么一段时间要学会自己长大。更多新东方VIP留...   \n",
       "\n",
       "                                                 target  \n",
       "1998  so the most important thing is to be togetherI...  \n",
       "1999  What an amazingly hard working, professional a...  \n",
       "2000  Finally,people with you,often you can not thin...  \n",
       "2001  we are young, we may want several love experie...  \n",
       "2002  gets tired.No one can take the pain for you. Y...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "# Load and shuffle data\n",
    "weiboDf= pd.DataFrame(weiboDict).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.576 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['成功', '不是', '一个', '结果', '，', '而是', '一个', '过程']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in chTexts:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "# Store all English text in a single file \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTexts: \n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece\n",
    "\n",
    "https://github.com/huggingface/tokenizers\n",
    "\n",
    "In the following cell, we train a `SentencePiece` tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1580, 3858, 1902, 1113, 1010, 2048] ['▁成功', '不是一个', '结果', ',而是', '一个', '过程'] [(0, 2), (2, 6), (6, 8), (8, 11), (11, 13), (13, 15)] [1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\myTokenizer-vocab.json', '.\\\\myTokenizer-merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "chTokenizer.train([pathAllCh], \n",
    "                vocab_size = 20000, \n",
    "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = chTokenizer.encode(chTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "chTokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to encode a list of texts as a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁成功', '不是一个', '结果', ',而是', '一个', '过程']\n",
      "['▁我', '都要', '好好的', '对自己', '爱自己', '。', '好好', '对', '待', '自己,', '为了', '明天', '所', '给予的', '礼', ',而不是', '昨天', '所', '带', '走', '的', '记']\n",
      "['▁约', '时报', '称,', '朝', '体', '发布', '的', '部分', '金正日', '礼', '照片', '曾', '用', 'P', 'ho', 't', 'os', 'ho', 'p', '处理', '过。']\n"
     ]
    }
   ],
   "source": [
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: padding and truncation\n",
    "\n",
    "Huggingface tokenizer allows us to pad or truncate according to a length. The following are common utilities for padding and truncation: \n",
    "\n",
    "`Tokenizer.enable_padding(**args)` -- Enable padding\n",
    "\n",
    "`Tokenizer.padding` -- Info about padding\n",
    "\n",
    "`Tokenizer.no_padding()` -- Disable padding\n",
    "\n",
    "`Tokenizer.enable_truncation(**args)` -- Enable truncation \n",
    "\n",
    "`Tokenizer.truncation` -- Info about truncation \n",
    "\n",
    "`Tokenizer.no_truncation()` -- Disable truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁成功', '不是一个', '结果', ',而是', '一个', '过程', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['▁我', '都要', '好好的', '对自己', '爱自己', '。', '好好', '对', '待', '自己,', '为了', '明天', '所', '给予的', '礼', ',而不是', '昨天', '所', '带', '走', '的', '记']\n",
      "['▁约', '时报', '称,', '朝', '体', '发布', '的', '部分', '金正日', '礼', '照片', '曾', '用', 'P', 'ho', 't', 'os', 'ho', 'p', '处理', '过。']\n",
      "{'length': 15, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    }
   ],
   "source": [
    "# With Padding\n",
    "chTokenizer.enable_padding(length = 15)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁成功', '不是一个', '结果', ',而是', '一个', '过程']\n",
      "['▁我', '都要', '好好的', '对自己', '爱自己', '。', '好好', '对', '待', '自己,', '为了', '明天', '所', '给予的', '礼', ',而不是', '昨天', '所', '带', '走', '的', '记']\n",
      "['▁约', '时报', '称,', '朝', '体', '发布', '的', '部分', '金正日', '礼', '照片', '曾', '用', 'P', 'ho', 't', 'os', 'ho', 'p', '处理', '过。']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No padding\n",
    "chTokenizer.no_padding()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁成功', '不是一个', '结果']\n",
      "['▁我', '都要', '好好的']\n",
      "['▁约', '时报', '称,']\n",
      "{'max_length': 3, 'stride': 0, 'strategy': 'longest_first'}\n"
     ]
    }
   ],
   "source": [
    "# With truncation\n",
    "chTokenizer.enable_truncation(max_length = 3)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁成功', '不是一个', '结果', ',而是', '一个', '过程']\n",
      "['▁我', '都要', '好好的', '对自己', '爱自己', '。', '好好', '对', '待', '自己,', '为了', '明天', '所', '给予的', '礼', ',而不是', '昨天', '所', '带', '走', '的', '记']\n",
      "['▁约', '时报', '称,', '朝', '体', '发布', '的', '部分', '金正日', '礼', '照片', '曾', '用', 'P', 'ho', 't', 'os', 'ho', 'p', '处理', '过。']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No truncation\n",
    "chTokenizer.no_truncation()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Pending problem.</span> As I tried to follow the tutorial and load the tokenizer saved on disk, unexpected error was reported. For now, skip loading saved tokenizer and proceed with other important steps.  \n",
    "\n",
    "<span style=\"color:red;\">Bottleneck for now.</span> Do we need special token for T5? If yes, how to insert special T5 tokens into our tokenization? Similar to `tokenizers.processors.BertProcessing`, do we have `tokenizers.processors.T5Processing`? \n",
    "\n",
    "<span style=\"color:red;\">Solution.</span> 1. Thoroughtly read documentation for T5 model in huggingface doc; 2. Explore `huggingface/tokenizers` library on github. \n",
    "\n",
    "For now, halt with tokenizer and proceed with language model until bumping into problems. Keep in mind the confusion about special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2538, 898, 938, 988, 1238, 1530, 1812, 865, 2777, 20] ['▁Success', '▁is', '▁not', '▁an', '▁even', 't.', '▁It’s', '▁a', '▁journey', '.'] [(0, 7), (7, 10), (10, 14), (14, 17), (17, 22), (22, 24), (24, 29), (29, 31), (31, 39), (39, 40)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "enTokenizer.train([pathAllEn], \n",
    "                vocab_size = 20000, \n",
    "               special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = enTokenizer.encode(enTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "# tokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before training\n",
    "\n",
    "To utilize PyTorch and GPU computation, we need to create instances of `Dataset` object. \n",
    "\n",
    "`DataLoader` class allows us to iterate a dataset with given batch size. We will define dataloaders in `T5FineTuner` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell overwrites `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Load original data and util from memory or file\n",
    "    Texts must be passed as lists \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 chTexts, enTexts, # Suppose the two colums are of the same length\n",
    "                 chTokenizer, enTokenizer, \n",
    "                 chMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        chOutputs = chTokenizer.encode(chTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        chEncoding = chOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        chMask = chOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(chEncoding).to(device), \n",
    "            'source_mask': torch.tensor(chMask).to(device), \n",
    "            'target_ids': torch.tensor(enEncoding).to(device), \n",
    "            'target_mask': torch.tensor(enMask).to(device)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "chMaxLen = 100\n",
    "enMaxLen = 100\n",
    "\n",
    "dataset = MyDataset(chTexts[:1500], enTexts[:1500], \n",
    "                    chTokenizer, enTokenizer, \n",
    "                    chMaxLen = chMaxLen, enMaxLen = enMaxLen)\n",
    "\n",
    "print(len(dataset))\n",
    "# print(dataset.__getitem__(0))\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "PyTorch native, despite its great flexibility, may trap you in detailed errors that mess up the entire code. For example, you may forget important details like `optimizer.zero_grad()` or `tensor.to(device)` in PyTorch native. For both learning purpose and clarity in the long run, we use `pytorch_lightning` to define model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams): \n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.chTokenizer = hparams['chTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        # self.rouge_metric = nlp.load_metric('rouge')\n",
    "        \n",
    "        # No idea what the \"freeze\" is doing\n",
    "        if self.hparams['freeze_embeds']:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams['freeze_encoder']:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "            \n",
    "            \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask = None, \n",
    "                decoder_input_ids = None, \n",
    "                decoder_attention_mask = None, \n",
    "                lm_labels = None\n",
    "               ): \n",
    "        # Type `Seq2SeqLMOutput`\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            lm_labels = lm_labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Prepare optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        model = self.model \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # model.named_parameters() can't find doc?\n",
    "                'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay': self.hparams['weight_decay']\n",
    "            }, \n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr = self.hparams['learning_rate']\n",
    "        )\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    \n",
    "    # Override this method to adjust how Trainer calls each optimizer \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure = None, on_tpu = False, using_native_amp = False, using_lbfgs = False):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    # Why do we set zero_grad at this moment? \n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 4: Define training logic\n",
    "    -- In PyTorch native, we have to manually define the epoch loop, define the batch loop, and manually perform model.train(), loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    # subroutine for training_step()\n",
    "    def _step(self, batch): \n",
    "        lm_labels = batch['target_ids']    # !! Does not apply! \n",
    "        lm_labels[lm_labels[:, ] == 0] = -100    # !! Verify that id for pad is 0 \n",
    "         \n",
    "        # !! There is a `__call__` method associated with self ?! \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'],    # !! Does not apply! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            lm_labels = lm_labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss    # !! Or should it be outputs[0] ? \n",
    "    \n",
    "    \n",
    "    # Called at the end of training epoch \n",
    "    # Do something with all the outputs from every training step \n",
    "    def training_epoch_end(self, outputs): \n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'avg_train_loss': avg_train_loss}\n",
    "        return {\n",
    "            'avg_train_loss': avg_train_loss, \n",
    "            'log': tensorboard_logs, \n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 5: Define validation logic\n",
    "    -- In PyTorch native, we have to define the batch loop, and manually perform model.eval(), torch.no_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        return self._generative_step(batch)\n",
    "    \n",
    "    # subroutine for validation_step()\n",
    "    def _generative_step(self, batch): \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # !! model.generate() Can't find doc !\n",
    "        generated_ids = self.model.generate(\n",
    "            batch['source_ids'],    # !! Does not apply ! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            use_cache = True, \n",
    "            decoder_attention_mask = batch['target_mask'],     # !! Does not apply! \n",
    "            max_length = self.hparams['max_output_len'],\n",
    "            num_beams = 2,     # ?? What is this?\n",
    "            repetition_penalty = 2.5,     # ?? What is this?\n",
    "            length_penalty = 1.0,     # ?? What is this?\n",
    "            early_stopping = True    # ?? What is this?\n",
    "        )\n",
    "        \n",
    "        preds = self.ids_to_clean_text(enTokenizer, generated_ids)    # translation predicted by model \n",
    "        target = self.ids_to_clean_text(enTokenizer, batch['target_ids'])    # !! Does not apply! \n",
    "        \n",
    "        gen_time = (time.time() - t0) / batch['source_ids'].shape[0]    # !! Does not apply\n",
    "        \n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        # Compute metrics\n",
    "        # ?? What is the deal with \"rouge\" in the code example? \n",
    "        base_metrics = {'val_loss': loss}\n",
    "        trans_len = np.mean(list(map(len, generated_ids)))\n",
    "        base_metrics.update(\n",
    "            gen_time = gen_time, \n",
    "            gen_len = trans_len, \n",
    "            preds = preds, \n",
    "            target = target\n",
    "        )\n",
    "        # self.rouge_metric.add_batch(preds, target)\n",
    "        \n",
    "        return base_metrics\n",
    "        \n",
    "    \n",
    "    #\n",
    "    def validation_epoch_end(self, outputs): \n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        \n",
    "        # rouge_results = self.rouge_metric.compute()\n",
    "        # rouge_dict = self.parse_score(rouge_results)\n",
    "        # tensorboard_logs.update(rouge1 = rouge_dict['rouge1'], rougeL = rouge_dict['rougeL'])\n",
    "        \n",
    "        # Clear out the lists for next epoch \n",
    "        # !! I don't see those two variables defined anywhere \n",
    "        self.target_gen = []\n",
    "        self.prediction_gen = []\n",
    "        \n",
    "        return {\n",
    "            'avg_val_loss': avg_loss, \n",
    "            # 'rouge1': rouge_results['rouge1'], \n",
    "            # 'rougeL': rouge_results['rougeL'], \n",
    "            'log': tensorboard_logs,\n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''Part 6: Define dataloaders'''\n",
    "    def train_dataloader(self): \n",
    "        train_dataset = get_dataset(\n",
    "            chTexts = chTexts[:1800], \n",
    "            enTexts = enTexts[:1800], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = self.hparams['train_batch_size'], \n",
    "            drop_last = True, \n",
    "            shuffle = True, \n",
    "            num_workers = 0 \n",
    "        )\n",
    "        \n",
    "        # The code below deals with scheduler. And I have no idea what the code is doing \n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams['train_batch_size'] * max(1, self.hparams['n_gpu'])))\n",
    "            // self.hparams['gradient_accumulation_steps']\n",
    "            * float(self.hparams['num_train_epochs'])\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams['warmup_steps'], num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            chTexts = chTexts[1800:1950], \n",
    "            enTexts = enTexts[1800:1950], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size = self.hparams['eval_batch_size'], \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = get_dataset(\n",
    "            chTexts = chTexts[1950:], \n",
    "            enTexts = enTexts[1950:], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size = self.hparams['eval_batch_size'], \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' ==================================\n",
    "    # Collection of helper functions \n",
    "    # Not predefined by LightningModule\n",
    "    ===================================='''\n",
    "    \n",
    "    # Decode a batch of ids and return a list of strings \n",
    "    def ids_to_clean_text(self, tokenizer, ids_batch): \n",
    "        ids_batch_tensor = torch.tensor(ids_batch)\n",
    "        # Make sure that the ids come as a batch and that decode_batch() method will work properly \n",
    "        assert (ids_batch_tensor.ndim >= 2), 'Ids do not form a batch'\n",
    "        return tokenizer.decode_batch(ids_batch.tolist())\n",
    "            \n",
    "        \n",
    "    # tqdm is a library for showing progress bar \n",
    "    # Retrieve info needed for progresse bar \n",
    "    def get_tqdm_dict(self): \n",
    "        # !! What is self.trainer? I never saw it defined \n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.3f}'.format(self.trainer.avg_loss), \n",
    "            'lr': self.lr_scheduler.get_last_lr()[-1]\n",
    "        }\n",
    "        return tqdm_dict\n",
    "    \n",
    "    '''=================================\n",
    "    # Methods that I have no idea what they are doing \n",
    "    =================================='''\n",
    "    def freeze_params(self, model):\n",
    "        for par in model.parameters():\n",
    "            par.requires_grad = False\n",
    "            \n",
    "            \n",
    "    def freeze_embeds(self):\n",
    "        # Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\n",
    "        try:\n",
    "            self.freeze_params(self.model.model.shared)\n",
    "            for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "                freeze_params(d.embed_positions)\n",
    "                freeze_params(d.embed_tokens)\n",
    "        except AttributeError:\n",
    "            self.freeze_params(self.model.shared)\n",
    "            for d in [self.model.encoder, self.model.decoder]:\n",
    "                self.freeze_params(d.embed_tokens)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparamsDict = {\n",
    "    'chTokenizer': chTokenizer, \n",
    "    'enTokenizer': enTokenizer, \n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'weight_decay': 0.0, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'train_batch_size': 8, \n",
    "    'eval_batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'n_gpu': 1\n",
    "    # For now, we do train-test split manually when defining dataloader, instead of loading the following param \n",
    "    # 'n_train': 2000\n",
    "    # 'n_val': 150\n",
    "    # 'n_test': 50\n",
    "}\n",
    "\n",
    "# ?? What are these hyperparameters ? \n",
    "hparamsDictNoUnderstand = {\n",
    "    'freeze_encoder': False, \n",
    "    'freeze_embeds': False, \n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'resume_from_checkpoint': None, \n",
    "    'val_check_interval': 0.05,\n",
    "    'early_stop_callback': False, \n",
    "    'fp_16': False, \n",
    "    'opt_level': 'O1', \n",
    "    'max_grad_norm': 1.0, \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "hparamsDict.update(hparamsDictNoUnderstand)\n",
    "\n",
    "hparams = argparse.Namespace(**hparamsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea what the following code cells are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        # if pl_module.is_logger():\n",
    "        metrics = trainer.callback_metrics\n",
    "        # Log results\n",
    "        for key in sorted(metrics):\n",
    "            if key not in [\"log\", \"progress_bar\"]:\n",
    "                logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        # if pl_module.is_logger():\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        # Log and save results to file\n",
    "        output_test_results_file = \"./test_results.txt\"\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Checkpoint directory ./ exists and is not empty. With save_top_k=3, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Set up wandb \n",
    "os.environ[\"WANDB_API_KEY\"] = '4a50a6213c69c6a669deb96f81ced074ecac908a'\n",
    "wandb_logger = WandbLogger(project='ch-en-try')\n",
    "\n",
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath='./', prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
    ")\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=hparams['gradient_accumulation_steps'],\n",
    "    gpus=hparams['n_gpu'],\n",
    "    max_epochs=hparams['num_train_epochs'], \n",
    "    # early_stop_callback=False,\n",
    "    precision= 16 if hparams['fp_16'] else 32,\n",
    "    amp_level=hparams['opt_level'],\n",
    "    resume_from_checkpoint=hparams['resume_from_checkpoint'],\n",
    "    gradient_clip_val=hparams['max_grad_norm'], \n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    val_check_interval=hparams['val_check_interval'],\n",
    "    # logger=wandb_logger,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen):\n",
    "    return MyDataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1038f6ad24ab4f499f8ff5359f80c76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-31210a68b2d5>:257: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ids_batch_tensor = torch.tensor(ids_batch)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\transformers\\modeling_t5.py:1144: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573f308a95464d75af94cb130b7bb546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f05486bbd4d48bb86447ff2bc6bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776bf36112e44136b6ed22549176b2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29acf20c3944b989a9df41959660c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a22660aeeaf493ab2d6b0a13ae93683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6885c9ce54b4b0bb85a72dc6ade14ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41246a1abb94f129cecee2be1a263a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52643f4b9ffa4669849e756d5dabcce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c581556126b24a6bbb6c51aade00d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee19a8766b154850851acc115d2d20d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe9e009a81740b59ca128dd8ca12d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9009c53fe25f4cbd9c0241aa16bac8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1dd55225284b118ea7da117b19ef41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050da4122f174adcb96e4170cb7ea91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322fccb0ea7b481594d899f70a6a565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb90b042d42140fda29fcc12359c5de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5234b6809475abbf2b8842188989d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe42ff6ee5f4213b5d954595c84dbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a1c32a1d9f4f56b07c9ef92cb3d219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f5717f43c14ee0a8b9e0f70efed3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d38f79d52074a18b6abe3958da9e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ac9750450f4ed9bedbc98a9144bed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63f1cc43cbd4815a028c3539423a7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d808ab66a1014fa6a820e8fd696ae89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7548d9518d4964bfe2572b19c47117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7069fb51c034442807e5d87d05ea825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a829727628a49d7bc12ef9264a38659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a9d17b38043e584d8dd2ab136ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0bb6f5ded74dcdabd2ba8801940f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b14a05dd9d43838e3987c00630471d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dfac7e4b9f45388686fb5b58bfc08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a705d1fe244e0a8dc34bccf0fb1303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b2ef34d5ce4cc48f82f178318b766c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e11ac6efc4408987de170a9e44bc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d7b0a173a446f1be58a289a5c1bf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841fc750da874061a8d276fc1826b3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b544e92de74fe6aeab333f73e7a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1b19d6f9164e2780f8fd082a3652e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6a2ecd4d3c4b338638197fbe353c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea239f8461f040f09a7948acc5d57cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25180f93cf344f19fe4376b1bec1211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5FineTuner(hparams)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_checkpoint(\"t5_ch_en_1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved model \n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path=\"t5_ch_en_1.ckpt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap \n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "datashow = get_dataset(\n",
    "            chTexts = chTexts, \n",
    "            enTexts = enTexts, \n",
    "            chTokenizer = chTokenizer, \n",
    "            enTokenizer = enTokenizer, \n",
    "            chMaxLen = hparamsDict['max_input_len'], \n",
    "            enMaxLen = hparamsDict['max_output_len']\n",
    "        )\n",
    "\n",
    "loader = DataLoader(datashow, batch_size = 32)\n",
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask=batch['source_mask'].cuda(),\n",
    "    use_cache=True,\n",
    "    decoder_attention_mask=batch['target_mask'].cuda(),\n",
    "    max_length = hparamsDict['max_output_len'], \n",
    "    num_beams = 2, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    early_stopping = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Text: 成功不是一个结果,而是一个过程\n",
      "\n",
      "Actual translation: Success is not an event. It’s a journey.\n",
      "\n",
      ".esdicted translation: 1#:!)\n",
      " ready the奢/\"持校 depend hill性ose8』&丽A士婷%3R>房,4Nf\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 我都要好好的对自己爱自己。好好对待自己,为了明天所给予的礼,而不是昨天所带走的记\n",
      "\n",
      "Actual translation: Treat yourself well. Live for what tomorrow has to offer, not for what yesterday took away.\n",
      "\n",
      "#1Kdicted translation: !.,)4\n",
      "5:+M7Ye乎NB上ʌfg*\"&>A奢%3-』T\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 约时报称,朝体发布的部分金正日礼照片曾用Photoshop处理过。\n",
      "\n",
      "Actual translation: Korea's state news agency has transmitted altered photographs of the funeral procession for the late leader Kim Jong-il held in Pyongyang on Wednesday, New York Times reported.\n",
      "\n",
      ".,)471*eB:+f&Yg\" the奢'3!#\n",
      "俐-乎K〜=>上ʌN↖M―s付O%E<J佳8A持/』T\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 新一年,本人祝大家,年快乐\n",
      "\n",
      "Actual translation: ,一帆风顺.Happy new year\n",
      "\n",
      ".,41#cted translation: )!\n",
      "&\"奢'3EA*%-j“rom儀>b。2JPŋ』<T\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 现在是学习的时间还是午餐的时间?小朋友们可以在这个故事中看到自己生活的影子吧?高频词\n",
      "\n",
      "Actual translation: for, is, it, the ;书中单词:breakfast, bus, home, lunch, math, music, reading, recess, school, science, time\n",
      "\n",
      ".,1)4:e7+ translation: #!\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 最好的朋友,能你在门默默地千,彼此没有只字片语,分别时却让你感到,这是你拥有的最棒的一次交流.\n",
      "\n",
      "Actual translation: The best kind of friend is the one you could sit on a porch swing with, never say a word, and then walk away feeling like it was the best conversation you've ever had.\n",
      "\n",
      "*,4:K7&Nd translation: )!.#+1\n",
      "eʌfYg\"MB上乎=♦>↖〜丁不-8%5E'3/A the奢\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 我又要啊!【安全很可爱】这次是\n",
      "\n",
      "Actual translation: Hello kitty condoms\n",
      "\n",
      "L4+dicted translation: #1,!).B:\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 在自己最快、最荣耀、最悲痛时想到的那个人,就是你永远最爱的人。来自\n",
      "\n",
      "Actual translation: In the most enjoyable and most glorious, when the saddest thought of that person, that you’ll always love the most.\n",
      "\n",
      ".,1)*eted translation: !#\n",
      " the奢&\"』3EAar-/J校持%5:47+\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 我想知道你有没有片刻想起过我,因为我一直在想你。来自\n",
      "\n",
      "Actual translation: I wonder if I ever cross your mind, for me it happens all the time.\n",
      "\n",
      ".,)!*\"ted translation: #\n",
      "%3E&>A奢/J the性51:+e-74Yf\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 我要永远健康。\n",
      "\n",
      "Actual translation: God ,you must bless me always healthy.\n",
      "\n",
      "5#ftend+*文translation: !.<1end sil\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 我觉得生命是一份礼物,我不想浪费它,你不会知道下一手会是什么,要学会接受生活——《TITANIC泰克号\n",
      "\n",
      "Actual translation: figure life is a gift and I don't intend on wasting it. You never know what hand you're going to get dealt next. You learn to take life as it comes at you.\n",
      "\n",
      "-;J>icted translation: #E\"奢\n",
      "?!'任口,〜%[&*A3R你』8.)4P先b。佳7+KO/ the性校2<两Uhŋ而arT\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 承诺再多,都做不到。那也只不过还是言\n",
      "\n",
      "Actual translation: Promise more, do not. It is just a lie\n",
      "\n",
      "\"*!%&.,)d translation: #\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 写给2013的自己\n",
      "\n",
      "Actual translation: Also write to myself!\n",
      "\n",
      ".,14:)e7+W*\"&nslation: !#\n",
      "-8奢%EHar'J the性k』3P>A...very ready to, those tired reach持读ise/士婷娜哥a ably d Live始译 who;j en激 wildG`问萌R丽书今“ about2i两v仪♋✈人<h5儀 mindl号劲暴[’~~S运台词adyM\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 谢谢帮我打房间的朋友。终于加入单身快乐的\"百菜帮\"啦\n",
      "\n",
      "Actual translation: A friend in need is the friend indeed.\n",
      "\n",
      "%』&Aicted translation: #\"!\n",
      ",1).B:'3;*?而奢-/`来校持婷 the性<R>b。iPE♋D8J佳T\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 你是选择体验快乐,还是选择创造快乐,我们只有两种选择。\n",
      "\n",
      "Actual translation: Would you like to experiment with happy ,or you select to creat happiness ,there are two ways for you in your life..\n",
      "\n",
      "%\"奢'*劲&Ej translation: ! than.,#\n",
      " We绕h. to...)婷持 U读校ar>性Kg1甲執 of暗/A始-b4 d Live哥译母 wond羞t.cheld nould失萌邮vice time运台词7+ Y乎:偷〜遇le儀仪3 mind ru⍛M床暴O<P important闹预俐♣feB丁歌8|\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 放弃不一定代表你的。有时候它意味着你强大到足以放手。\n",
      "\n",
      "Actual translation: let it go~Giving up doesn't always mean you are weak.Sometimes it means that you are strong enough to let go.\n",
      "\n",
      ".,+)icted translation: !1#\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 转发微博。【28:11】 富足人自以为有智慧。但明的人,能将他查透\n",
      "\n",
      "Actual translation: The rich man is wise in his own conceit; but the poor that hath understanding searcheth him out.\n",
      "\n",
      "%』3P>A持ed translation: !#奢. the性&\"*,\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 资网 ShoeBiz x Converse 2012 Chuck Taylor All-Star San Francisco位于金山知名鞋\n",
      "Shoebiz,不仅有大量限量的球鞋是所有鞋迷朝的地点以外,其本 .........\n",
      "\n",
      "Actual translation: ShoeBiz x Converse 2012 Chuck Taylor All-Star San Francisco - 新新球鞋网-\n",
      "\n",
      ".,)+1*e4: translation: !#\n",
      "7&\"奢' the性8』-%E[Jj>b/3Pŋ<A持\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: you never know who is falling in love with your smile. 然伤心,也不要不展,因为你不知是谁会爱上你的笑容\n",
      "\n",
      "Actual translation: Never frown, even when you are sad, because you never know who is falling in love with your smile.Never frown, even when you are sad, because\n",
      "\n",
      ".,1)4*\"ed translation: !#\n",
      "&e7+Y:MBfʌNg%-'A the奢8』3P邮婷持 ready$\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 今天最大的收获:找到了@资中 的微博。谢谢@张力奋\n",
      "的转。她在谈及四人帮时,提到的重要性。我认为就是每个人问自己的心,而不是将手指指向除自己以外的每个人,能做到了,就能无师自通\n",
      "\n",
      "Actual translation: Enlightenment is all about ruthless soul-searching, to rediscover one's own humanness.\n",
      "\n",
      ".,)+147:e translation: !#\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 这话就座右好了。世界太大,生命这样短,要把它过得尽量像自己想要的那个样子。\n",
      "\n",
      "Actual translation: The world is so big, and life is so short. We need to do as much as possible to live our lives the way we want to.\n",
      "\n",
      ".1),4e+7:YNK*BfʌMtion: !#\n",
      "↖&\"奢'甲 is住icant易one淑房執\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 市警方打击组出卖人体器犯罪 已抓获犯罪疑人137名\n",
      "\n",
      "Actual translation: China nabs 137 for organizing organ sale\n",
      "\n",
      "而持edicted translation: !3\"奢A the\n",
      "「E&.,)4*J校'』-%;<2R>b。\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 在你让别人做事情之前,先设身处地为他们想想。\n",
      "\n",
      "Actual translation: you put somebody in their place, put yourself in their place.\n",
      "\n",
      "%\"edicted translation: !\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 有时候,你需要做的就是嘴,放下所谓的自尊,承认自己的错误。这不叫放弃,而叫成长。 >>>(关注\n",
      "\n",
      "Actual translation: you gotta shut up, swallow your pride and accept that you're wrong. It's not giving up. It's called growing up.\n",
      "\n",
      "%\"奢'&>*K1:4ranslation: )!.,#\n",
      "? the校-』Bg7O8婷持/J歌\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 和这两个日巨头周四公布的业表明,它们仍有很长的路要走。今日30%至1974年来最低点\n",
      "\n",
      "Actual translation: Announcements of dismal earnings from stalwarts Sony and Sharp on Thursday showed the firms still have a long way to go. Sharp slumped 30% today to its lowest level since 1974\n",
      "\n",
      "&\"奢'*cted translation: #.,)!\n",
      "14:+e7Bg-K%』3P>AarT\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: don't mean I don't care.—— 有些事,我不说,我不问,不代表我不在乎\n",
      "\n",
      "Actual translation: don't,I don't ask\n",
      "\n",
      ".) far'\"法*,1+nslation: !#\n",
      "\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 不疑你的每一天,我的永都比你的更好。 你得上帝不你?其正默默你...\n",
      "\n",
      "Actual translation: GOD: And don't doubt that My plan for your day is Always Better than your plan.\n",
      "\n",
      ".,#)icted translation: !\n",
      "e1-\"*&A the奢持 ready校读%3;婷士ly U哥/劲 to萌Ej佳'J性ose8』5P mindb凶k<R>H7:Y4ʌBg+K\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 生活就是这个样子\n",
      "\n",
      "Actual translation: Life's like this.\n",
      "\n",
      "煊!W%\"*,ed translation: :\n",
      "C两E&D-'A3P止.4#呵 Korearow$\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: ,生日因为你们变不同\n",
      "\n",
      "Actual translation: happy birthday,\n",
      "\n",
      "\"奢t..cted translation: ,\n",
      "&!*%-Eb the性8』ŋ3P◎A劲T\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 爱情,要么让人成长,要么让人沉。\n",
      "\n",
      "Actual translation: Love makes man grow up or sink down.\n",
      "\n",
      "*\"易one yours.nslation: !,\n",
      "\n",
      "房&.%5#奢'/任呵 dr beautifulnd性)+1:e\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 把剧本写好。把声音录好。把演员选好。你一定会做出一部好电影的\n",
      "\n",
      "Actual translation: Get the script right. Get the sound right. Get the actors right. You'll make a good movie.\n",
      "\n",
      ".e+dicted translation: !#)14\n",
      ",7:YNfgBʌ MK=*\"婷奢持 ready the性&>b台词凶8 U读校 reachady俐sV付变丁哥娜 to劲%-/Aar'』3P仪♦5EJose\n",
      "=====================================================================\n",
      "\n",
      "Chinese Text: 必须要做但很无或很难的工作;吃力不讨好的工作2.\n",
      "\n",
      "Actual translation: get down to work 开始做重要的事情;完成重要的任务3. make short work of something 很快完成 4. work like a horse\n",
      "\n",
      "\".,#&奢'*%-8ranslation: !\n",
      "\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "\n",
    "texts = [chTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "targets = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(32):\n",
    "    lines = textwrap.wrap(\"Chinese Text:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % targets[i])\n",
    "    print(\"\\nPredicted translation: %s\" % preds[i])\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
