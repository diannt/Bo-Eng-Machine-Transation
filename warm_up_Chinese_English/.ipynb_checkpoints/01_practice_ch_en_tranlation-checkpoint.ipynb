{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation\n",
    "\n",
    "Huggingface transformer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Huggingface tokenizer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Useful resources from huggingface -- fine-tuning a model from scratch: https://huggingface.co/blog/how-to-train\n",
    "\n",
    "The code I wrote before might be helpful: https://github.com/submal/ctec-lambus/blob/master/xprmt/xprmt_06.ipynb\n",
    "\n",
    "A code example of fine-tuning T5 for text summarization: https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81 \n",
    "\n",
    "LighningModule API\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-apihttps://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import nlp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3384767877756162</td>\n",
       "      <td>我不是没脾气，只是不轻易发脾气。我不是不爱你，只是放在心里而已</td>\n",
       "      <td>did not temper，but not easily loses his temper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3532149993122332</td>\n",
       "      <td>人生只有走出来的美丽，没有等出来的辉煌</td>\n",
       "      <td>perfect[哈哈]The beauty of life is only made by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3560829729419834</td>\n",
       "      <td>巧虎，生日因为你们变不同</td>\n",
       "      <td>happy birthday,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3534107223123680</td>\n",
       "      <td>朋友就是和ta在一起你可以大胆做自己的人</td>\n",
       "      <td>You are my friend[心]A friend is someone with w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>3401340888150445</td>\n",
       "      <td>现在的你的位置并不重要，重要的是你前进的方向。方向 对了，继续加油吧！明天一定会更好</td>\n",
       "      <td>It's not where you are today that counts. It's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                      source  \\\n",
       "1998  3384767877756162             我不是没脾气，只是不轻易发脾气。我不是不爱你，只是放在心里而已   \n",
       "1999  3532149993122332                         人生只有走出来的美丽，没有等出来的辉煌   \n",
       "2000  3560829729419834                                巧虎，生日因为你们变不同   \n",
       "2001  3534107223123680                        朋友就是和ta在一起你可以大胆做自己的人   \n",
       "2002  3401340888150445  现在的你的位置并不重要，重要的是你前进的方向。方向 对了，继续加油吧！明天一定会更好   \n",
       "\n",
       "                                                 target  \n",
       "1998  did not temper，but not easily loses his temper...  \n",
       "1999  perfect[哈哈]The beauty of life is only made by ...  \n",
       "2000                                    happy birthday,  \n",
       "2001  You are my friend[心]A friend is someone with w...  \n",
       "2002  It's not where you are today that counts. It's...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "# Load and shuffle data\n",
    "weiboDf= pd.DataFrame(weiboDict).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.523 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好', '的', '爱情', '使', '你', '通过', '一个', '人', '看到', '整个', '世界', '，', '坏', '的', '爱情', '使', '你', '为了', '一个', '人', '舍弃', '整个', '世界', '。']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in chTexts:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "# Store all English text in a single file \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTexts: \n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece\n",
    "\n",
    "https://github.com/huggingface/tokenizers\n",
    "\n",
    "In the following cell, we train a `SentencePiece` tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6952, 1491, 2253, 1036, 1157, 2196, 3008, 339, 2036, 1491, 1197, 1036, 5766, 4104, 94] ['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。'] [(0, 4), (4, 6), (6, 8), (8, 11), (11, 13), (13, 15), (15, 18), (18, 19), (19, 22), (22, 24), (24, 26), (26, 29), (29, 31), (31, 35), (35, 36)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\myTokenizer-vocab.json', '.\\\\myTokenizer-merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "chTokenizer.train([pathAllCh], \n",
    "                vocab_size = 20000, \n",
    "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = chTokenizer.encode(chTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "chTokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to encode a list of texts as a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n"
     ]
    }
   ],
   "source": [
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: padding and truncation\n",
    "\n",
    "Huggingface tokenizer allows us to pad or truncate according to a length. The following are common utilities for padding and truncation: \n",
    "\n",
    "`Tokenizer.enable_padding(**args)` -- Enable padding\n",
    "\n",
    "`Tokenizer.padding` -- Info about padding\n",
    "\n",
    "`Tokenizer.no_padding()` -- Disable padding\n",
    "\n",
    "`Tokenizer.enable_truncation(**args)` -- Enable truncation \n",
    "\n",
    "`Tokenizer.truncation` -- Info about truncation \n",
    "\n",
    "`Tokenizer.no_truncation()` -- Disable truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "{'length': 15, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    }
   ],
   "source": [
    "# With Padding\n",
    "chTokenizer.enable_padding(length = 15)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No padding\n",
    "chTokenizer.no_padding()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过']\n",
      "['▁如果你', '真', '要梦想']\n",
      "['▁有时候,', '最好的', '方法就是']\n",
      "{'max_length': 3, 'stride': 0, 'strategy': 'longest_first'}\n"
     ]
    }
   ],
   "source": [
    "# With truncation\n",
    "chTokenizer.enable_truncation(max_length = 3)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁好的爱情', '使你', '通过', '一个人', '看到', '整个', '世界,', '坏', '的爱情', '使你', '为了', '一个人', '舍弃', '整个世界', '。']\n",
      "['▁如果你', '真', '要梦想', '成真', ',就', '先', '从梦中', '醒来', '。']\n",
      "['▁有时候,', '最好的', '方法就是', '什么', '也不', '做']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No truncation\n",
    "chTokenizer.no_truncation()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Pending problem.</span> As I tried to follow the tutorial and load the tokenizer saved on disk, unexpected error was reported. For now, skip loading saved tokenizer and proceed with other important steps.  \n",
    "\n",
    "<span style=\"color:red;\">Bottleneck for now.</span> Do we need special token for T5? If yes, how to insert special T5 tokens into our tokenization? Similar to `tokenizers.processors.BertProcessing`, do we have `tokenizers.processors.T5Processing`? \n",
    "\n",
    "<span style=\"color:red;\">Solution.</span> 1. Thoroughtly read documentation for T5 model in huggingface doc; 2. Explore `huggingface/tokenizers` library on github. \n",
    "\n",
    "For now, halt with tokenizer and proceed with language model until bumping into problems. Keep in mind the confusion about special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2101, 961, 1407, 874, 1145, 875, 1771, 1080, 1103, 1016, 1140, 1581, 1459, 961, 1407, 874, 3558, 875, 1771, 1080, 925, 1016, 1140, 20] ['▁Good', '▁love', '▁makes', '▁you', '▁see', '▁the', '▁whole', '▁world', '▁from', '▁one', '▁person', '▁while', '▁bad', '▁love', '▁makes', '▁you', '▁abandon', '▁the', '▁whole', '▁world', '▁for', '▁one', '▁person', '.'] [(0, 4), (4, 9), (9, 15), (15, 19), (19, 23), (23, 27), (27, 33), (33, 39), (39, 44), (44, 48), (48, 55), (55, 61), (61, 65), (65, 70), (70, 76), (76, 80), (80, 88), (88, 92), (92, 98), (98, 104), (104, 108), (108, 112), (112, 119), (119, 120)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "enTokenizer.train([pathAllEn], \n",
    "                vocab_size = 20000, \n",
    "               special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = enTokenizer.encode(enTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "# tokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before training\n",
    "\n",
    "To utilize PyTorch and GPU computation, we need to create instances of `Dataset` object. \n",
    "\n",
    "`DataLoader` class allows us to iterate a dataset with given batch size. We will define dataloaders in `T5FineTuner` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell overwrites `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Load original data and util from memory or file\n",
    "    Texts must be passed as lists \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 chTexts, enTexts, # Suppose the two colums are of the same length\n",
    "                 chTokenizer, enTokenizer, \n",
    "                 chMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        chOutputs = chTokenizer.encode(chTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        chEncoding = chOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        chMask = chOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(chEncoding).to(device), \n",
    "            'source_mask': torch.tensor(chMask).to(device), \n",
    "            'target_ids': torch.tensor(enEncoding).to(device), \n",
    "            'target_mask': torch.tensor(enMask).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "chMaxLen = 100\n",
    "enMaxLen = 100\n",
    "\n",
    "dataset = MyDataset(chTexts[:1500], enTexts[:1500], \n",
    "                    chTokenizer, enTokenizer, \n",
    "                    chMaxLen = chMaxLen, enMaxLen = enMaxLen)\n",
    "\n",
    "print(len(dataset))\n",
    "# print(dataset.__getitem__(0))\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "PyTorch native, despite its great flexibility, may trap you in detailed errors that mess up the entire code. For example, you may forget important details like `optimizer.zero_grad()` or `tensor.to(device)` in PyTorch native. For both learning purpose and clarity in the long run, we use `pytorch_lightning` to define model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams): \n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams.pretrainedModelName, \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.chTokenizer = hparams.chTokenizer\n",
    "        self.enTokenizer = hparams.enTokenizer\n",
    "        self.rouge_metric = nlp.load_metric('rouge')\n",
    "        \n",
    "        # No idea what the \"freeze\" is doing\n",
    "        if self.hparams.freeze_embed:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams.freeze_encoder:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "            \n",
    "            \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask = None, \n",
    "                decoder_input_ids = None, \n",
    "                decoder_attention_mask = None, \n",
    "                lm_labels = None\n",
    "               ): \n",
    "        # Type `Seq2SeqLMOutput`\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            lm_labels = lm_labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Prepare optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        model = self.model \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # model.named_parameters() can't find doc?\n",
    "                'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay': self.hparams.weight_decay\n",
    "            }, \n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr = self.hparams.learning_rate\n",
    "        )\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    \n",
    "    # Override this method to adjust how Trainer calls each optimizer \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure = None, using_native_amp = False): \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    # Why do we set zero_grad at this moment? \n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 4: Define training logic\n",
    "    -- In PyTorch native, we have to manually define the epoch loop, define the batch loop, and manually perform model.train(), loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    # subroutine for training_step()\n",
    "    def _step(self, batch): \n",
    "        lm_labels = batch['target_ids']    # !! Does not apply! \n",
    "        lm_labels[lm_lables[:, ] == 0] = -100    # !! Verify that id for pad is 0 \n",
    "         \n",
    "        # !! There is a `__call__` method associated with self ?! \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'],    # !! Does not apply! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            lm_labels = lm_labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss    # !! Or should it be outputs[0] ? \n",
    "    \n",
    "    \n",
    "    # Called at the end of training epoch \n",
    "    # Do something with all the outputs from every training step \n",
    "    def training_epoch_end(self, outputs): \n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'avg_train_loss': avg_train_loss}\n",
    "        return {\n",
    "            'avg_train_loss': avg_train_loss, \n",
    "            'log': tensorboard_logs, \n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 5: Define validation logic\n",
    "    -- In PyTorch native, we have to define the batch loop, and manually perform model.eval(), torch.no_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        return self._generative_step(batch)\n",
    "    \n",
    "    # subroutine for validation_step()\n",
    "    def _generative_step(self, batch): \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # !! model.generate() Can't find doc !\n",
    "        generated_ids = self.model.generate(\n",
    "            batch['source_ids'],    # !! Does not apply ! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            use_cache = True, \n",
    "            decoder_attention_mask = batch['target_mask'],     # !! Does not apply! \n",
    "            max_length = hparam['max_output_len'],\n",
    "            num_beams = 2,     # ?? What is this?\n",
    "            repetition_penalty = 2.5,     # ?? What is this?\n",
    "            length_penalty = 1.0,     # ?? What is this?\n",
    "            early_stopping = True    # ?? What is this?\n",
    "        )\n",
    "        \n",
    "        preds = self.ids_to_clean_text(generated_ids)    # translation predicted by model \n",
    "        target = self.ids_to_clean_text(batch['target_ids'])    # !! Does not apply! \n",
    "        \n",
    "        gen_time = (time.time() - t0) / batch['source_ids'].shape[0]    # !! Does not apply\n",
    "        \n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        # Compute metrics\n",
    "        # ?? What is the deal with \"rouge\" in the code example? \n",
    "        base_metrics = {'val_loss': loss}\n",
    "        trans_len = np.mean(list(map(len, generated_ids)))\n",
    "        base_metrics.update(\n",
    "            gen_time = gen_time, \n",
    "            gen_len = trans_len, \n",
    "            preds = preds, \n",
    "            target = target\n",
    "        )\n",
    "        self.rouge_metric.add_batch(preds, target)\n",
    "        \n",
    "        return base_metrics\n",
    "        \n",
    "    \n",
    "    #\n",
    "    def validation_epoch_end(self, outputs): \n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        \n",
    "        rouge_results = self.rouge_metric.compute()\n",
    "        rouge_dict = self.parse_score(rouge_results)\n",
    "        \n",
    "        tensorboard_logs.update(rouge1 = rouge_dict['rouge1'], rougeL = rouge_dict['rougeL'])\n",
    "        \n",
    "        # Clear out the lists for next epoch \n",
    "        # !! I don't see those two variables defined anywhere \n",
    "        self.target_gen = []\n",
    "        self.prediction_gen = []\n",
    "        \n",
    "        return {\n",
    "            'avg_val_loss': avg_loss, \n",
    "            'rouge1': rouge_results['rouge1'], \n",
    "            'rougeL': rouge_results['rougeL'], \n",
    "            'log': tensorboard_logs,\n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''Part 6: Define dataloaders'''\n",
    "    def train_dataloader(self): \n",
    "        train_dataset = MyDataset(\n",
    "            chTexts = chTexts[:1800], \n",
    "            enTexts = enTexts[:1800], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.max_input_len, \n",
    "            enMaxLen = self.max_output_len\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = self.hparams.train_batch_size, \n",
    "            drop_last = True, \n",
    "            shuffle = True, \n",
    "            num_workers = 4\n",
    "        )\n",
    "        \n",
    "        # The code below deals with scheduler. And I have no idea what the code is doing \n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = MyDataset(\n",
    "            chTexts = chTexts[1800:1950], \n",
    "            enTexts = enTexts[1800:1950], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.max_input_len, \n",
    "            enMaxLen = self.max_output_len\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size = self.hparams.eval_batch_size, \n",
    "            num_workers = 4\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = MyDataset(\n",
    "            chTexts = chTexts[1950:], \n",
    "            enTexts = enTexts[1950:], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.max_input_len, \n",
    "            enMaxLen = self.max_output_len\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size = self.hparams.eval_batch_size, \n",
    "            num_workers = 4\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' ==================================\n",
    "    # Collection of helper functions \n",
    "    # Not predefined by LightningModule\n",
    "    ===================================='''\n",
    "    \n",
    "    # Decode a batch of ids and return a list of strings \n",
    "    def ids_to_clean_text(self, tokenizer, ids_batch): \n",
    "        ids_batch_tensor = torch.tensor(ids_batch)\n",
    "        # Make sure that the ids come as a batch and that decode_batch() method will work properly \n",
    "        assert (id_batch_tensor.ndim >= 2), 'Ids do not form a batch'\n",
    "        return tokenizer.decode_batch(ids_batch.tolist())\n",
    "            \n",
    "        \n",
    "    # tqdm is a library for showing progress bar \n",
    "    # Retrieve info needed for progresse bar \n",
    "    def get_tqdm_dict(self): \n",
    "        # !! What is self.trainer? I never saw it defined \n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.3f}'.format(self.trainer.avg_loss), \n",
    "            'lr': self.lr_scheduler.get_last_lr()[-1]\n",
    "        }\n",
    "        return tqdm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparamsDict = {\n",
    "    'chTokenizer': chTokenizer, \n",
    "    'enTokenizer': enTokenizer, \n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'weight_decay': 0.0, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'train_batch_size': 8, \n",
    "    'eval_batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'n_gpu': 1\n",
    "    # For now, we do train-test split manually when defining dataloader, instead of loading the following param \n",
    "    # 'n_train': 2000\n",
    "    # 'n_val': 150\n",
    "    # 'n_test': 50\n",
    "}\n",
    "\n",
    "# ?? What are these hyperparameters ? \n",
    "hparamsDictNoUnderstand = {\n",
    "    'freeze_encoder': False, \n",
    "    'freeze_embed': False, \n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_stpes': 0,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'resume_from_checkpoint': None, \n",
    "    'val_check_interval': 0.05,\n",
    "    'early_stop_callback': False, \n",
    "    'fp_16': False, \n",
    "    'opt_level': 'O1', \n",
    "    'max_grad_norm': 1.0, \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "hparamsDict.update(hparamsDictNoUnderstand)\n",
    "\n",
    "hparams = argparse.Namespace(**hparamsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea what the following code cells are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "            # Log and save results to file\n",
    "            output_test_results_file = \"./test_results.txt\"\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-da300c93351c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Set up wandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"WANDB_API_KEY\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'4a50a6213c69c6a669deb96f81ced074ecac908a'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwandb_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWandbLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ch-en-try'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## Define Checkpoint function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, save_dir, offline, id, anonymous, version, project, log_model, experiment, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     ):\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwandb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise ImportError('You want to use `wandb` logger which is not installed yet,'  # pragma: no-cover\n\u001b[0m\u001b[0;32m     86\u001b[0m                               ' install it with `pip install wandb`.')\n\u001b[0;32m     87\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`."
     ]
    }
   ],
   "source": [
    "## Set up wandb \n",
    "os.environ[\"WANDB_API_KEY\"] = '4a50a6213c69c6a669deb96f81ced074ecac908a'\n",
    "wandb_logger = WandbLogger(project='ch-en-try')\n",
    "\n",
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath='./', prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
    ")\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=hparams.gradient_accumulation_steps,\n",
    "    gpus=hparams.n_gpu,\n",
    "    max_epochs=hparams.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if hparams.fp_16 else 32,\n",
    "    amp_level=hparams.opt_level,\n",
    "    resume_from_checkpoint=hparams.resume_from_checkpoint,\n",
    "    gradient_clip_val=hparams.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    val_check_interval=hparams.val_check_interval,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
