{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3477898759956095</td>\n",
       "      <td>能发现自己的错误是智慧，能改正自己的错误是勇敢。喜欢请关注</td>\n",
       "      <td>Can find their own mistakes is wisdom, to corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3526354320111701</td>\n",
       "      <td>我的死后日願望是，可以有世界末日啦！@browNsugaR 我在:</td>\n",
       "      <td>Very good music in Suns [good] //@browNsugaR:E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3558553027867596</td>\n",
       "      <td>年后，让你觉得更失望的不是你做过的事情，而是你没有做过的事情。所以，一直想做的事情，不要再拖了</td>\n",
       "      <td>online#Twenty years from now you will be more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3482173493644453</td>\n",
       "      <td>说：“在小寨军区侧门，有人维权。过来个军车就给人家看。不过貌似四医大不归省军区管吧</td>\n",
       "      <td>It's hard for her to win,she should explicit m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>3562778084139688</td>\n",
       "      <td>没有受伤，不懂坚强；不犯错误，难以成长；未曾失败，何来成功。</td>\n",
       "      <td>Day78：You'll never be brave if you don't get h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                           source  \\\n",
       "1998  3477898759956095                    能发现自己的错误是智慧，能改正自己的错误是勇敢。喜欢请关注   \n",
       "1999  3526354320111701                我的死后日願望是，可以有世界末日啦！@browNsugaR 我在:   \n",
       "2000  3558553027867596  年后，让你觉得更失望的不是你做过的事情，而是你没有做过的事情。所以，一直想做的事情，不要再拖了   \n",
       "2001  3482173493644453        说：“在小寨军区侧门，有人维权。过来个军车就给人家看。不过貌似四医大不归省军区管吧   \n",
       "2002  3562778084139688                   没有受伤，不懂坚强；不犯错误，难以成长；未曾失败，何来成功。   \n",
       "\n",
       "                                                 target  \n",
       "1998  Can find their own mistakes is wisdom, to corr...  \n",
       "1999  Very good music in Suns [good] //@browNsugaR:E...  \n",
       "2000  online#Twenty years from now you will be more ...  \n",
       "2001  It's hard for her to win,she should explicit m...  \n",
       "2002  Day78：You'll never be brave if you don't get h...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "weiboDf = pd.DataFrame(weiboDict)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.533 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好', '的', '爱情', '使', '你', '通过', '一个', '人', '看到', '整个', '世界', '，', '坏', '的', '爱情', '使', '你', '为了', '一个', '人', '舍弃', '整个', '世界', '。']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in weiboDf['source']:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5Tokenizer' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-82aec5ff60f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't5-small'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"<pad>\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'T5Tokenizer' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, BertTokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
